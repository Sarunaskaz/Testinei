{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarun\\OneDrive\\Documents\\Code\\Testinei\\mokymai\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 7.9992\n",
      "Epoch 2/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 7.3424\n",
      "Epoch 3/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 7.0341\n",
      "Epoch 4/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 6.7048\n",
      "Epoch 5/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 6.2763\n",
      "Epoch 6/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 5.8388\n",
      "Epoch 7/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 5.3690\n",
      "Epoch 8/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 4.8337\n",
      "Epoch 9/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 4.3118\n",
      "Epoch 10/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 3.8321\n",
      "Epoch 11/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 3.3868\n",
      "Epoch 12/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 2.9671\n",
      "Epoch 13/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 2.6156\n",
      "Epoch 14/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 2.3062\n",
      "Epoch 15/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 2.0285\n",
      "Epoch 16/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 1.7890\n",
      "Epoch 17/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 1.5512\n",
      "Epoch 18/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 1.3447\n",
      "Epoch 19/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 1.1564\n",
      "Epoch 20/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.9917\n",
      "Epoch 21/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.8500\n",
      "Epoch 22/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.7386\n",
      "Epoch 23/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.6276\n",
      "Epoch 24/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.5242\n",
      "Epoch 25/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.4501\n",
      "Epoch 26/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.3707\n",
      "Epoch 27/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.3128\n",
      "Epoch 28/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.2618\n",
      "Epoch 29/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.2163\n",
      "Epoch 30/30\n",
      "\u001b[1m1594/1594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.1835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2192ef3a870>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "with open(\"tekstas.txt\", \"r\", encoding='UTF-8') as f:\n",
    "  data = f.read()\n",
    "\n",
    "tokenizer = Tokenizer() # sukuriame tokenizer objekta, kuris konvertuos teksta i skaitinius zodziu indeksus\n",
    "tokenizer.fit_on_texts([data]) # mokome tokenizer su duomenimis\n",
    "seqeunces = tokenizer.texts_to_sequences([data])[0]\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1 # deklaruojame kiek zodziu mes turesime savo zodyne, jo veliau reikes nurodant isejimo galimybes\n",
    "\n",
    "sequence_length = 5\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in range(sequence_length, len(seqeunces)): # uzpildome sarasus zodziu sekomis ir zodziais, einanciais po ju\n",
    "    x.append(seqeunces[i-sequence_length:i])\n",
    "    y.append(seqeunces[i])\n",
    "\n",
    "x = np.array(x) # konvertuojame x i matrica\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=sequence_length),\n",
    "    LSTM(128),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit(x, y, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Šiandien apniukusi diena dėl to pusę kai nusileido ir viską vėl gerai arkliai kad proto kliuvo – matai pajuosčio keista jurga pasirengdamas yra gandai tėveli tiktai kartą ir nuo akis žiūrėjo o jis būtų jos pasidaręs – atsiliepė baltaragis apsidžiaugęs raupys nueiti gal taip ir nešlovė viską viena pat koja nuo pelkių pavasario davatkyne kaip\n"
     ]
    }
   ],
   "source": [
    "pradinis_tekstas = 'Šiandien apniukusi diena dėl to'\n",
    "\n",
    "for _ in range(50):\n",
    "    words_list = tokenizer.texts_to_sequences([pradinis_tekstas])[0]\n",
    "    spejimo_sequence = np.array(words_list[-sequence_length:])\n",
    "    x = np.expand_dims(spejimo_sequence, axis=0)\n",
    "    predicted = model.predict(x, verbose=0)\n",
    "    predicted_index = np.argmax(predicted, axis=1)[0]\n",
    "    word = tokenizer.index_word[predicted_index]\n",
    "    pradinis_tekstas += ' ' + word\n",
    "\n",
    "print(pradinis_tekstas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mokymai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
